{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Abusive Language Detection Dataset\n",
    "data1 = pd.read_csv('abusive tweets/abusive language detection/Tweets.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['label', 'tweet'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data1.columns = ['label', 'tweet']\n",
    "print(data1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Only among Muslims can one find someone proudl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>@_sabanaqvi Only among Muslims can one find so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@megha_writes Muslim rapist ?\", \"truncated\": f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>@peoplepower @ACLU A Muslim holding a placard ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>RT @Pad_Ban: Bohemians PC. Home to AFA thugs a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                              tweet\n",
       "0      0  Only among Muslims can one find someone proudl...\n",
       "1      0  @_sabanaqvi Only among Muslims can one find so...\n",
       "2      0  @megha_writes Muslim rapist ?\", \"truncated\": f...\n",
       "3      0  @peoplepower @ACLU A Muslim holding a placard ...\n",
       "4      0  RT @Pad_Ban: Bohemians PC. Home to AFA thugs a..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label     int64\n",
       "tweet    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique categories in the label column: [0, 1, 2, 3, 4]\n",
      "Categories (5, int64): [0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "# Convert label to categorical type\n",
    "data1['label'] = data1['label'].astype('category')\n",
    "\n",
    "# Display the unique categories\n",
    "print(\"Unique categories in the label column:\", data1['label'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Logistic Regression ===\n",
      "Cross-validation scores: [0.41550191 0.64972469 0.70478611 0.57983905 0.44260906]\n",
      "Mean cross-validation score: 0.5584921643371452\n",
      "\n",
      "Accuracy: 0.6937738246505718\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.78      0.79       478\n",
      "           1       0.55      0.54      0.54       474\n",
      "           2       0.88      0.83      0.86       480\n",
      "           3       0.63      0.69      0.66       461\n",
      "           4       0.63      0.62      0.62       468\n",
      "\n",
      "    accuracy                           0.69      2361\n",
      "   macro avg       0.70      0.69      0.69      2361\n",
      "weighted avg       0.70      0.69      0.70      2361\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split data into features (X) and target (y)\n",
    "X = data1['tweet']\n",
    "y = data1['label']\n",
    "\n",
    "# Vectorize text data\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_vectorized = vectorizer.fit_transform(X)\n",
    "print(\"\\n=== Logistic Regression ===\")\n",
    "\n",
    "# Initialize Logistic Regression model\n",
    "model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(model, X_vectorized, y, cv=5)\n",
    "\n",
    "# Display cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean cross-validation score:\", cv_scores.mean())\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Model evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"\\nAccuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters, URLs, and mentions\n",
    "    text = re.sub(r'http\\S+|www\\S+|@[^\\s]+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Lemmatize words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # Join tokens back into text\n",
    "    text = ' '.join(tokens)\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the background, this function takes raw text as input and applies a series of text preprocessing steps to clean and normalize the text data. These steps include converting text to lowercase, removing special characters, URLs, and mentions, tokenizing the text into words, removing stop words, lemmatizing words, and finally joining the processed tokens back into text format. This clean and preprocessed text can then be used for further analysis or modeling tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sivac\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess the tweet text\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Naive Bayes (MultinomialNB) ===\n",
      "Cross-validation scores: [0.33883947 0.6154172  0.69589157 0.54807285 0.40152478]\n",
      "Mean cross-validation score: 0.51994917407878\n",
      "\n",
      "Accuracy: 0.6552308343922066\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.80      0.72       478\n",
      "           1       0.51      0.54      0.53       474\n",
      "           2       0.88      0.76      0.82       480\n",
      "           3       0.59      0.68      0.63       461\n",
      "           4       0.71      0.48      0.57       468\n",
      "\n",
      "    accuracy                           0.66      2361\n",
      "   macro avg       0.67      0.65      0.65      2361\n",
      "weighted avg       0.67      0.66      0.65      2361\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize model\n",
    "nb_model = MultinomialNB()\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(nb_model, X_vectorized, y, cv=5)\n",
    "print(\"\\n=== Naive Bayes (MultinomialNB) ===\")\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean cross-validation score:\", cv_scores.mean())\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = nb_model.predict(X_test)\n",
    "\n",
    "# Model evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"\\nAccuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Support Vector Machine (SVM) ===\n",
      "Cross-validation scores: [0.4180432  0.66073698 0.71452774 0.59042778 0.44176197]\n",
      "Mean cross-validation score: 0.5650995340957221\n",
      "\n",
      "Accuracy: 0.7132570944515036\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.79      0.80       478\n",
      "           1       0.56      0.61      0.58       474\n",
      "           2       0.93      0.83      0.88       480\n",
      "           3       0.67      0.70      0.68       461\n",
      "           4       0.64      0.63      0.63       468\n",
      "\n",
      "    accuracy                           0.71      2361\n",
      "   macro avg       0.72      0.71      0.72      2361\n",
      "weighted avg       0.72      0.71      0.72      2361\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize model\n",
    "svm_model = SVC(probability=True)\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(svm_model, X_vectorized, y, cv=5)\n",
    "print(\"\\n=== Support Vector Machine (SVM) ===\")\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean cross-validation score:\", cv_scores.mean())\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Model evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"\\nAccuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted level of abuse: Moderate Level of Abuse\n"
     ]
    }
   ],
   "source": [
    "#tweet_text = 'Only among Muslims can one find someone proudly cooing that a rapist, murderer, and misogynist'\n",
    "#tweet_text = \"I love spending time with my family. #happy\"\n",
    "#tweet_text = \"I can't believe he said that to me. So rude!\"\n",
    "tweet_text = \"fuck you slut \"\n",
    "\n",
    "preprocessed_tweet = preprocess_text(tweet_text)\n",
    "\n",
    "# Vectorize the preprocessed tweet using the same vectorizer used during training\n",
    "vectorized_tweet = vectorizer.transform([preprocessed_tweet])\n",
    "\n",
    "# Make prediction probabilities on the vectorized tweet\n",
    "predicted_probabilities = svm_model.predict_proba(vectorized_tweet)[0]\n",
    "\n",
    "# Get the category with the highest probability\n",
    "predicted_category_index = np.argmax(predicted_probabilities)\n",
    "\n",
    "# Map predicted category index to its corresponding label reflecting the level of abuse\n",
    "category_mapping = {\n",
    "    4: 'No Abuse',\n",
    "    3: 'Low Level of Abuse',\n",
    "    2: 'Moderate Level of Abuse',\n",
    "    1: 'High Level of Abuse',\n",
    "    0: 'Very High Level of Abuse'\n",
    "}\n",
    "predicted_label = category_mapping[predicted_category_index]\n",
    "\n",
    "# Print the predicted label\n",
    "print(\"Predicted level of abuse:\", predicted_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['svm_model.joblib']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "\n",
    "# Save the trained SVM model\n",
    "dump(svm_model, 'svm_model.joblib')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
