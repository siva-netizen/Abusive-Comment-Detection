{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import  matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOADING DATA  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv(\"youtube-spam-collection-v1/Youtube01-Psy.csv\") \n",
    "data2 = pd.read_csv(\"youtube-spam-collection-v1/Youtube02-KatyPerry.csv\")\n",
    "data3 = pd.read_csv(\"youtube-spam-collection-v1/Youtube03-LMFAO.csv\")\n",
    "data4 = pd.read_csv(\"youtube-spam-collection-v1/Youtube04-Eminem.csv\")\n",
    "data5 = pd.read_csv(\"youtube-spam-collection-v1/Youtube05-Shakira.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1951</th>\n",
       "      <td>_2viQ_Qnc6-bMSjqyL1NKj57ROicCSJV5SwTrw-RFFA</td>\n",
       "      <td>Katie Mettam</td>\n",
       "      <td>2013-07-13T13:27:39.441000</td>\n",
       "      <td>I love this song because we sing it at Camp al...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1952</th>\n",
       "      <td>_2viQ_Qnc6-pY-1yR6K2FhmC5i48-WuNx5CumlHLDAI</td>\n",
       "      <td>Sabina Pearson-Smith</td>\n",
       "      <td>2013-07-13T13:14:30.021000</td>\n",
       "      <td>I love this song for two reasons: 1.it is abou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1953</th>\n",
       "      <td>_2viQ_Qnc6_k_n_Bse9zVhJP8tJReZpo8uM2uZfnzDs</td>\n",
       "      <td>jeffrey jules</td>\n",
       "      <td>2013-07-13T12:09:31.188000</td>\n",
       "      <td>wow</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1954</th>\n",
       "      <td>_2viQ_Qnc6_yBt8UGMWyg3vh0PulTqcqyQtdE7d4Fl0</td>\n",
       "      <td>Aishlin Maciel</td>\n",
       "      <td>2013-07-13T11:17:52.308000</td>\n",
       "      <td>Shakira u are so wiredo</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1955</th>\n",
       "      <td>_2viQ_Qnc685RPw1aSa1tfrIuHXRvAQ2rPT9R06KTqA</td>\n",
       "      <td>Latin Bosch</td>\n",
       "      <td>2013-07-12T22:33:27.916000</td>\n",
       "      <td>Shakira is the best dancer</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       COMMENT_ID                AUTHOR  \\\n",
       "1951  _2viQ_Qnc6-bMSjqyL1NKj57ROicCSJV5SwTrw-RFFA          Katie Mettam   \n",
       "1952  _2viQ_Qnc6-pY-1yR6K2FhmC5i48-WuNx5CumlHLDAI  Sabina Pearson-Smith   \n",
       "1953  _2viQ_Qnc6_k_n_Bse9zVhJP8tJReZpo8uM2uZfnzDs         jeffrey jules   \n",
       "1954  _2viQ_Qnc6_yBt8UGMWyg3vh0PulTqcqyQtdE7d4Fl0        Aishlin Maciel   \n",
       "1955  _2viQ_Qnc685RPw1aSa1tfrIuHXRvAQ2rPT9R06KTqA           Latin Bosch   \n",
       "\n",
       "                            DATE  \\\n",
       "1951  2013-07-13T13:27:39.441000   \n",
       "1952  2013-07-13T13:14:30.021000   \n",
       "1953  2013-07-13T12:09:31.188000   \n",
       "1954  2013-07-13T11:17:52.308000   \n",
       "1955  2013-07-12T22:33:27.916000   \n",
       "\n",
       "                                                CONTENT  CLASS  \n",
       "1951  I love this song because we sing it at Camp al...      0  \n",
       "1952  I love this song for two reasons: 1.it is abou...      0  \n",
       "1953                                                wow      0  \n",
       "1954                            Shakira u are so wiredo      0  \n",
       "1955                         Shakira is the best dancer      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.concat([data1,data2,data3,data4,data5])\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "data.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1956.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.513804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.499937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             CLASS\n",
       "count  1956.000000\n",
       "mean      0.513804\n",
       "std       0.499937\n",
       "min       0.000000\n",
       "25%       0.000000\n",
       "50%       1.000000\n",
       "75%       1.000000\n",
       "max       1.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output is the result of calling the `describe()` method on a DataFrame named `data`, specifically on the 'CLASS' column. Here's what you can infer from this output:\n",
    "\n",
    "1. **Count**: There are 1956 entries in the 'CLASS' column.\n",
    "2. **Mean**: The mean value of the 'CLASS' column is approximately 0.514, indicating that about 51.4% of the entries are labeled as spam (assuming 1 represents spam and 0 represents non-spam).\n",
    "3. **Standard Deviation (std)**: The standard deviation is approximately 0.500, which indicates the dispersion of values around the mean. Since this is close to 0.5, it suggests that the data is somewhat evenly distributed around the mean.\n",
    "4. **Min**: The minimum value in the 'CLASS' column is 0, indicating the lowest label observed.\n",
    "5. **25th Percentile (25%)**: 25% of the data falls below this value, which is 0 in this case.\n",
    "6. **50th Percentile (Median or 50%)**: This is the median value, which is 1 in this case. It indicates that 50% of the data falls below this value, and 50% falls above it.\n",
    "7. **75th Percentile (75%)**: 75% of the data falls below this value, which is 1 in this case.\n",
    "8. **Max**: The maximum value in the 'CLASS' column is 1, indicating the highest label observed.\n",
    "\n",
    "Overall, this summary provides insights into the distribution and characteristics of the 'CLASS' column, which likely represents whether a particular entry in the dataset is classified as spam (1) or not spam (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1956, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['COMMENT_ID', 'AUTHOR', 'DATE', 'CONTENT', 'CLASS'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "COMMENT_ID      0\n",
       "AUTHOR          0\n",
       "DATE          245\n",
       "CONTENT         0\n",
       "CLASS           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking null value presence\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['DATE']=data['DATE'].ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "COMMENT_ID    0\n",
       "AUTHOR        0\n",
       "DATE          0\n",
       "CONTENT       0\n",
       "CLASS         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "COMMENT_ID    object\n",
       "AUTHOR        object\n",
       "DATE          object\n",
       "CONTENT       object\n",
       "CLASS          int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the data-types\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             CONTENT  CLASS\n",
      "0  Huh, anyway check out this you[tube] channel: ...      1\n",
      "1  Hey guys check out my new channel and our firs...      1\n",
      "2             just for test I have to say murdev.com      1\n",
      "3   me shaking my sexy ass on my channel enjoy ^_^ ﻿      1\n",
      "4            watch?v=vtaRGgvGtWQ   Check this out .﻿      1\n"
     ]
    }
   ],
   "source": [
    "# Select 'CONTENT' and 'CLASS' columns\n",
    "data = data[['CONTENT', 'CLASS']]\n",
    "\n",
    "# Verify the selection\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['047000', '0687119038', '09', '0cb8qfjaa', '100', '1000', '111982027348137311818', '12year', '17', '17yr', '18', '1990', '1bi', '2014', '2015', '23giraffebruuh', '23lmfao', '25000', '26', '26t22', '2b4wywphi8c', '2f', '2fen', '2flist_of_most_viewed_youtube_videos', '2fwiki', '30', '3000', '301', '33', '39', '3a', '3m', '3m57s', '4000dollars', '43', '447935454150', '45', '476000', '600m', '613000', '700', '710000', '73231344', '800', '832000', '868', '911', 'abbas', 'able', 'abominable', 'abuses', 'actual', 'actually', 'addicting', 'adf', 'admirable', 'adult', 'advance', 'afqjcngkm', 'africa', 'after', 'again', 'against', 'ago', 'agree', 'ahhh', 'airlines', 'alfred', 'ali', 'already', 'am', 'amazed', 'ambition', 'american', 'amiable', 'an', 'ana', 'and', 'animals', 'anthem', 'anyone', 'apologies', 'applause', 'applied', 'applocker', 'appreciate', 'are', 'arguements', 'arrogant', 'asinine', 'ask', 'asked', 'ass', 'attention', 'audio', 'autotune', 'autotuned', 'aware', 'awesom', 'baby', 'bad', 'balls', 'banging', 'based', 'basically', 'basketball', 'bastard', 'be', 'beat', 'beating', 'beats', 'because', 'becomes', 'being', 'belgium', 'believe', 'belly', 'belrus', 'beneath', 'bennett', 'better', 'bgq', 'bieber', 'bigger', 'billion', 'birthday', 'bitch', 'black', 'blank', 'bless', 'block', 'blows', 'boobs', 'book', 'boooobs', 'boring', 'bottom', 'box', 'boy', 'boys', 'br', 'brah', 'brand', 'breaken', 'brings', 'broken', 'brother', 'brothers', 'business', 'but', 'button', 'bv', 'bvm', 'cad', 'called', 'caroline', 'case', 'catchy', 'cazzy', 'cease', 'celeb', 'celebrated', 'celebrity', 'cge', 'chainise', 'chanel', 'channel', 'chaste', 'cheating', 'cheats', 'check', 'checking', 'cheers', 'cheetos', 'chillstep', 'chinese', 'ching', 'choice', 'chooses', 'christ', 'christianity', 'cking', 'clap', 'cleanse', 'clear', 'clicked', 'close', 'closer', 'clothes', 'club', 'cock', 'cold', 'collaborator', 'com', 'come', 'comeback', 'comeing', 'commercial', 'commit', 'complaints', 'complete', 'comprehend', 'computer', 'confidence', 'congrats', 'console', 'conveying', 'cool', 'copy', 'core', 'could', 'counsel', 'covered', 'crap', 'crash', 'crazy', 'credit', 'critiquing', 'croatia', 'cruz', 'curled', 'curse', 'cute', 'cyrus', 'da', 'daily', 'damn', 'dance', 'danced', 'dancing', 'danke', 'dante', 'deathly', 'deazy99', 'decent', 'dede', 'deep', 'deserve', 'details', 'dick', 'did', 'didn', 'die', 'ding', 'discrimination', 'disliked', 'disorder', 'divine', 'divorce', 'do', 'doesnt', 'domestic', 'don', 'done', 'dope', 'dot', 'download', 'dream', 'dreamers', 'dreams', 'dresses', 'dressprettyonce', 'drives', 'drunk', 'du', 'dudes', 'duh', 'during', 'earns', 'eat', 'eating', 'ei', 'either', 'elevator', 'elongate', 'em', 'eminem', 'eminems', 'eminmem', 'emotions', 'en', 'english', 'enjoy', 'enlargement', 'epic', 'equipment', 'erection', 'esrc', 'esteem', 'eu', 'everything', 'everywhere', 'evil', 'excuse', 'exist', 'exode', 'exposure', 'extraordinary', 'extremely', 'eyebrows', 'eyes', 'facebook', 'fack', 'faded', 'fair', 'fairrrrrrr', 'famous', 'fan', 'fangirls', 'fantastic', 'fashionable', 'fast', 'fathers', 'faves', 'fb', 'feeling', 'feels', 'fictional', 'figure', 'filled', 'fils', 'first', 'fish', 'five', 'flaming', 'floooooooooooooooooooop', 'follow', 'following', 'fool', 'foolish', 'fools', 'football', 'forget', 'form', 'formula', 'free', 'freestyle', 'ft', 'fu', 'fuck', 'fucked', 'fucken', 'fuckin', 'fucking', 'future', 'g2jvinpuemo', 'gained', 'ganga', 'gangnam', 'gardner', 'garnered', 'gay', 'george', 'giant', 'gimme', 'giraffe', 'giraffebruuh', 'girl', 'girls', 'gives', 'giving', 'god', 'going', 'gold', 'goo', 'good', 'google', 'gook', 'goonrock', 'gorg', 'got', 'gotta', 'gotten', 'government', 'grace', 'gracias', 'grazie', 'great', 'green', 'growing', 'hackfbaccountlive', 'had', 'half', 'hallows', 'hands', 'happening', 'happy', 'harry', 'harts', 'has', 'hashtag', 'hate', 'have', 'hay', 'he', 'head', 'heart', 'heaven', 'hell', 'helped', 'helpful', 'her', 'heroin', 'herself', 'hi', 'high', 'hilarious', 'him', 'his', 'hit', 'hiya', 'hog', 'holy', 'honesty', 'hook', 'hooker', 'hoping', 'hostile', 'hot', 'house', 'how', 'http', 'https', 'huge', 'human', 'humanity', 'humankind', 'hyuna', 'ibit', 'idiocy', 'idiots', 'idplal6kuvkoekvgdtt2jvlq', 'if', 'ignore', 'illuminati', 'im', 'impossible', 'impress', 'in', 'inbox', 'inc', 'inch', 'inches', 'incredible', 'industry', 'innocent', 'insane', 'insanely', 'inspiration', 'instagram', 'invented', 'invest', 'inviolate', 'is', 'isn', 'isnt', 'issue', 'it', 'its', 'ja', 'jackson', 'jam', 'jap', 'jb', 'jenny', 'jesus', 'just', 'justin', 'katty', 'keep', 'keeps', 'khalifa', 'kill', 'killed', 'kingston', 'kluivert', 'know', 'knowledge', 'known', 'koean', 'kollektivet', 'korean', 'kq6zr6kcpj8', 'lacked', 'laughing', 'lauren', 'leeched', 'legit', 'leisure', 'lets', 'lie', 'liers', 'life', 'like', 'liked', 'ling', 'lion', 'listen', 'lives', 'living', 'lmao', 'lmfao', 'lneadw26bfst76vhkjl8pxaey6vmnlvmriudtsfk6vy', 'lneadw26bft', 'lneadw26bftvzqt6juehasiefrjg1exi_dvqdnqvpho', 'lneadw26bfunoarag71awgu6tjo6azdkfiun_tz1_hy', 'lneadw26bfvkahxpkenm25fywkyxthsupri6juqsznu', 'long', 'looking', 'lord', 'lost', 'lot', 'love', 'loved', 'lovely', 'low', 'lucas', 'luck', 'lunden', 'lv', 'ly', 'lyric', 'lyrics', 'magically', 'mah', 'mahogany', 'makeing', 'man', 'many', 'mary', 'matters', 'may', 'maybe', 'mayby', 'maylaysia', 'mean', 'meaning', 'means', 'medication', 'meh', 'melody', 'memories', 'merci', 'message', 'mice', 'michael', 'might', 'mil', 'miley', 'millions', 'mind', 'minute', 'minutes', 'mio', 'mirror', 'miss', 'missing', 'missouri', 'mixtape', 'mockingbird', 'molly', 'money', 'moneygq', 'montageparodies', 'months', 'more', 'mosh', 'mother', 'mothers', 'moved', 'moves', 'msg', 'much', 'muhammad', 'murder', 'must', 'mxh2y77', 'nail', 'nature', 'needed', 'neighbor', 'new', 'newest', 'news', 'next', 'niggas', 'nigger', 'niko', 'no', 'nofollow', 'noise', 'none', 'notes', 'nothing', 'notorious', 'nude', 'numbering', 'nummber', 'nut', 'nuts', 'obrigado', 'obviously', 'of', 'off', 'official', 'oh', 'ok', 'old', 'older', 'oldspice', 'omg', 'on', 'once', 'one', 'onece', 'online', 'onqzzad3q3cmnbe9nml4ga', 'openly', 'oq3yu9dwc8l4yqoyo4goaw', 'oreo', 'ot', 'other', 'others', 'otib', 'otoufj0z3vfflfnaxykwzsivqhimx', 'our', 'ourself', 'out', 'outfit', 'over', 'overplayed', 'page', 'pages', 'parry', 'part', 'party', 'passionate', 'past', 'patrik', 'payhip', 'paša', 'pc', 'pcmasterace', 'penis', 'people', 'peoples', 'per', 'perfect', 'perhaps', 'perverse', 'pester', 'pewdiepie', 'php', 'piece', 'pigment', 'piss', 'pisses', 'place', 'plane', 'player', 'playing', 'playlist', 'please', 'plot', 'plus', 'plz', 'pn', 'point', 'porno', 'possible', 'posted', 'potter', 'power', 'practically', 'pray', 'present', 'press', 'pretty', 'problems', 'producers', 'professor', 'progressive', 'promise', 'prove', 'psy', 'pure', 'pushes', 'pussy', 'putty', 'queen', 'quiet', 'quot', 'radio', 'rage', 'rap', 'rape', 'rapper', 'raps', 'rate', 'rct', 'reaction', 'read', 'real', 'reason', 'receive', 'redeemer', 'ref', 'rel', 'relevant', 'religious', 'remove', 'repay', 'replay', 'research', 'resort', 'response', 'results', 'reveling', 'right', 'righteousness', 'rights', 'rihanna', 'riled', 'ring', 'rja', 'ro', 'roared', 'rockin', 'room', 'root', 'rule', 'sa', 'sadly', 'salon', 'salvation', 'same', 'samurman', 'sassy', 'satisfying', 'saved', 'savior', 'say', 'saying', 'says', 'screw', 'screwing', 'scrubs', 'sean', 'sec', 'second', 'secret', 'see', 'seen', 'self', 'selling', 'seriously', 'sex', 'sexual', 'sexy', 'shaeel', 'shaking', 'shakira', 'shall', 'she', 'shell', 'shhort', 'shit', 'shitty', 'shot', 'show', 'showcase', 'shrek', 'shy', 'sig2', 'silly', 'simple', 'simply', 'sinned', 'sins', 'sirius', 'sisters', 'sit', 'sitting', 'skin', 'skip', 'slappin', 'slash', 'slut', 'small', 'smile', 'so', 'soldiers', 'solve', 'some', 'sometime', 'son', 'song', 'songs', 'soo', 'sooo', 'sooooo', 'soooooooooooooooooooooooooooooooooooooooo', 'sounds', 'source', 'space', 'spammers', 'speaking', 'speaks', 'spent', 'spinal', 'splashes', 'spraytan', 'spread', 'srilanka', 'started', 'stay', 'stfu', 'stick', 'streets', 'strength', 'stuck', 'stupid', 'sty', 'style', 'sub', 'subs', 'subscribe', 'subscribed', 'success', 'suck', 'sucking', 'sucks', 'suddenly', 'supat', 'super', 'support', 'supt', 'surgery', 'surviving', 'suscribe', 'swag', 'swamp', 'switch', 'symbolic', 'take', 'taking', 'talent', 'talk', 'talking', 'tango', 'tapes', 'tend', 'tha', 'than', 'thankss', 'thanx', 'that', 'thats', 'the', 'then', 'there', 'therefore', 'these', 'they', 'thing', 'those', 'thou', 'thought', 'thousands', 'thru', 'thumbs', 'tiger', 'time', 'times', 'tinyurl', 'tits', 'to', 'toilet', 'too', 'topic', 'total', 'toy', 'tracks', 'train', 'translate', 'treating', 'trigo', 'trolls', 'true', 'truly', 'trumpetcallofgodonline', 'truth', 'trying', 'tsu', 'turn', 'tv', 'twin', 'twins', 'twitch', 'twitter', 'tyga', 'type', 'uk', 'uncle', 'undefiled', 'under', 'underground', 'understand', 'united', 'untanlted', 'until', 'untitled', 'up', 'updated', 'uphill', 'uploading', 'us', 'use', 'used', 'usg', 'utter', 'vanishes', 'ved', 'versace', 'very', 'vevo', 'video', 'viewed', 'views', 'vines', 'violence', 'virgin', 'visit', 'visits', 'vn', 'voice', 'vote', 'vulnerable', 'wages', 'waka', 'walmart', 'wan', 'wank', 'wanted', 'warning', 'wars', 'was', 'wat', 'watched', 'watchin', 'watching', 'water', 'wave', 'well', 'went', 'what', 'whats', 'when', 'which', 'while', 'white', 'whiz', 'who', 'whole', 'wholehearted', 'whore', 'wickedness', 'wife', 'wikipedia', 'will', 'win', 'wind', 'winners', 'wish', 'wited', 'with', 'worked', 'world', 'worth', 'worthless', 'would', 'wow', 'wright', 'wtf', 'wtp', 'wwhore', 'www', 'xx', 'xxx', 'yaaaaaa', 'yahoo', 'yeah', 'years', 'you', 'young', 'your', 'yourself', 'youtube', 'zonepa', 'οh']\n"
     ]
    }
   ],
   "source": [
    "# Install the better-profanity library\n",
    "# pip install better_profanity\n",
    "\n",
    "from better_profanity import profanity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "# TF-IDF vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(data['CONTENT'])\n",
    "\n",
    "# Profanity Detection\n",
    "def contains_profanity(text):\n",
    "    # Check if the text contains profanity\n",
    "    return profanity.contains_profanity(text)\n",
    "\n",
    "# Add a new column indicating if text contains profanity\n",
    "data['PROFANITY'] = data['CONTENT'].apply(contains_profanity)\n",
    "\n",
    "# Feature selection using SelectKBest and profanity detection\n",
    "k_best_profanity = SelectKBest(score_func=chi2, k=1000)\n",
    "X_selected_profanity = k_best_profanity.fit_transform(X_tfidf, data['PROFANITY'])\n",
    "\n",
    "# Get selected feature names\n",
    "selected_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "selected_indices_profanity = k_best_profanity.get_support(indices=True)\n",
    "selected_features_profanity = [selected_feature_names[i] for i in selected_indices_profanity]\n",
    "print(selected_features_profanity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 1 0 1 1 0 1 1 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1\n",
      " 1 0 0 0 1 0 1 1 0 1 1 0 1 0 0 1 1 0 0 1 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 1 0\n",
      " 1 0 1 0 0 0 0 0 1 0 1 1 1 1 1 1 0 1 1 0 0 0 1 0 0 1 1 1 1 0 0 0 1 1 1 1 0\n",
      " 1 0 0 0 0 0 1 0 1 1 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0\n",
      " 1 1 0 1 0 0 1 1 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 1 1 0 1 0 0 1\n",
      " 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1\n",
      " 0 1 0 0 0 1 1 0 0 0 0 1 0 0 1 1 0 0 1 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1\n",
      " 1 0 1 1 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 0 0 1 0\n",
      " 1 1 1 0 0 1 0 0 0 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 1 0 0 0 1 1 0 1 0 0 1 1 1 0 1 1 1 0 0 0 0 1\n",
      " 0 1 1 1 1 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0]\n",
      "Accuracy: 0.8979591836734694\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected_profanity, data['CLASS'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train a logistic regression model\n",
    "logistic_regression = LogisticRegression()\n",
    "logistic_regression.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = logistic_regression.predict(X_test)\n",
    "print(y_pred)\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(data['CONTENT'])\n",
    "y = np.array(data['CLASS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, r2_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, PrecisionRecallDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV = CountVectorizer()\n",
    "X = CV.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1956x4454 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 25765 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=13)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1564, 4454), (1564,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BNB = BernoulliNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BernoulliNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BernoulliNB</label><div class=\"sk-toggleable__content\"><pre>BernoulliNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "BernoulliNB()"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BNB.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = BNB.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,\n",
       "       1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,\n",
       "       0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,\n",
       "       0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8852040816326531"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"check this www.kkl.com hi bad ass i am your bitch .com\"\n",
    "test_data = CV.transform([test]).toarray()\n",
    "BNB.predict(test_data)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
